# OpenSSF CVE Benchmark for AI

README of the original repo can be found [here](https://github.com/ossf-cve-benchmark/ossf-cve-benchmark).

# About

This enhanced version of the benchmark addresses the problem that function level AI models face when they try to use
the original benchmark: function level alerts. The original benchmark is designed to compare static tools that produce
alerts at the line level. Usually the static tools are focusing on precision which generates a lot of false negatives
via not noticing vulnerabilities so they are not generating a lot of false positives which. But the AI driven models
(especially in the beginning phase of their training or development) are producing a lot of false positive alerts. This
enhanced version of the benchmark is designed to overcome this issue, so researchers can use it to compare static tools
with AI driven tools.

Furthermore, two new tools are added to the benchmark. One of them is DevSkim, which is a static analysis tool developed
by Microsoft. The other one is an example AI driven code analyser tool which can be used by researchers who want to test
their own vulnerability detector AI models, simply via replacing the existing model in the analyser with their
own models.

Also, two Dockerfiles and runner scripts are added to make it easier to run the benchmark, then analyse the results.

## Usage

Simply just run the `build_run_and_report.cmd` script. It will build the Dockerfiles, download all necessary files, run the benchmark, and generate the report.
When it finishes running, you can access the results at http://localhost:8080